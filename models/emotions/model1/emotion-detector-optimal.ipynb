{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","#for dirname, _, filenames in os.walk('/kaggle/input'):\n","    #for filename in filenames:\n","        #print(os.path.join(dirname, filename))\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","from tensorflow.keras.applications import VGG16, InceptionResNetV2\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["train_dir = \"../../../datasets/train/\" #passing the path with training images\n","test_dir = \"../../../datasets/test/\"   #passing the path with testing images"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["img_size = 48 #original size of the image"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Data Augmentation\n","--------------------------\n","rotation_range = rotates the image with the amount of degrees we provide\n","width_shift_range = shifts the image randomly to the right or left along the width of the image\n","height_shift range = shifts image randomly to up or below along the height of the image\n","horizontal_flip = flips the image horizontally\n","rescale = to scale down the pizel values in our image between 0 and 1\n","zoom_range = applies random zoom to our object\n","validation_split = reserves some images to be used for validation purpose\n","\"\"\"\n","\n","train_datagen = ImageDataGenerator(#rotation_range = 180,\n","                                         width_shift_range = 0.1,\n","                                         height_shift_range = 0.1,\n","                                         horizontal_flip = True,\n","                                         rescale = 1./255,\n","                                         #zoom_range = 0.2,\n","                                         validation_split = 0.2\n","                                        )\n","validation_datagen = ImageDataGenerator(rescale = 1./255,\n","                                         validation_split = 0.2)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] O sistema não pode encontrar o caminho especificado: '../../../datasets/train/'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn [5], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mApplying data augmentation to the images as we read \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mthem from their respectivve directories\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_generator \u001b[39m=\u001b[39m train_datagen\u001b[39m.\u001b[39;49mflow_from_directory(directory \u001b[39m=\u001b[39;49m train_dir,\n\u001b[0;32m      6\u001b[0m                                                     target_size \u001b[39m=\u001b[39;49m (img_size,img_size),\n\u001b[0;32m      7\u001b[0m                                                     batch_size \u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                                                     color_mode \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mgrayscale\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m                                                     class_mode \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m                                                     subset \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     11\u001b[0m                                                    )\n\u001b[0;32m     12\u001b[0m validation_generator \u001b[39m=\u001b[39m validation_datagen\u001b[39m.\u001b[39mflow_from_directory( directory \u001b[39m=\u001b[39m test_dir,\n\u001b[0;32m     13\u001b[0m                                                               target_size \u001b[39m=\u001b[39m (img_size,img_size),\n\u001b[0;32m     14\u001b[0m                                                               batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                                                               subset \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m                                                              )\n","File \u001b[1;32mc:\\Users\\F2315771\\.virtualenvs\\cis-face-detection\\lib\\site-packages\\keras\\preprocessing\\image.py:1650\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[1;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflow_from_directory\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     directory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1580\u001b[0m     keep_aspect_ratio\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m ):\n\u001b[0;32m   1582\u001b[0m     \u001b[39m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[0;32m   1583\u001b[0m \n\u001b[0;32m   1584\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1648\u001b[0m \u001b[39m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1650\u001b[0m     \u001b[39mreturn\u001b[39;00m DirectoryIterator(\n\u001b[0;32m   1651\u001b[0m         directory,\n\u001b[0;32m   1652\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1653\u001b[0m         target_size\u001b[39m=\u001b[39;49mtarget_size,\n\u001b[0;32m   1654\u001b[0m         color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[0;32m   1655\u001b[0m         keep_aspect_ratio\u001b[39m=\u001b[39;49mkeep_aspect_ratio,\n\u001b[0;32m   1656\u001b[0m         classes\u001b[39m=\u001b[39;49mclasses,\n\u001b[0;32m   1657\u001b[0m         class_mode\u001b[39m=\u001b[39;49mclass_mode,\n\u001b[0;32m   1658\u001b[0m         data_format\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_format,\n\u001b[0;32m   1659\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1660\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1661\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m   1662\u001b[0m         save_to_dir\u001b[39m=\u001b[39;49msave_to_dir,\n\u001b[0;32m   1663\u001b[0m         save_prefix\u001b[39m=\u001b[39;49msave_prefix,\n\u001b[0;32m   1664\u001b[0m         save_format\u001b[39m=\u001b[39;49msave_format,\n\u001b[0;32m   1665\u001b[0m         follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[0;32m   1666\u001b[0m         subset\u001b[39m=\u001b[39;49msubset,\n\u001b[0;32m   1667\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation,\n\u001b[0;32m   1668\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype,\n\u001b[0;32m   1669\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\F2315771\\.virtualenvs\\cis-face-detection\\lib\\site-packages\\keras\\preprocessing\\image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[1;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m classes:\n\u001b[0;32m    562\u001b[0m     classes \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 563\u001b[0m     \u001b[39mfor\u001b[39;00m subdir \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(directory)):\n\u001b[0;32m    564\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    565\u001b[0m             classes\u001b[39m.\u001b[39mappend(subdir)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] O sistema não pode encontrar o caminho especificado: '../../../datasets/train/'"]}],"source":["\"\"\"\n","Applying data augmentation to the images as we read \n","them from their respectivve directories\n","\"\"\"\n","train_generator = train_datagen.flow_from_directory(directory = train_dir,\n","                                                    target_size = (img_size,img_size),\n","                                                    batch_size = 64,\n","                                                    color_mode = \"grayscale\",\n","                                                    class_mode = \"categorical\",\n","                                                    subset = \"training\"\n","                                                   )\n","validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n","                                                              target_size = (img_size,img_size),\n","                                                              batch_size = 64,\n","                                                              color_mode = \"grayscale\",\n","                                                              class_mode = \"categorical\",\n","                                                              subset = \"validation\"\n","                                                             )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"\n","Modeling\n","\n","\n","model = Sequential()\n","model.add(Conv2D(filters = 64,kernel_size = (3,3),padding = 'same',activation = 'relu',input_shape=(img_size,img_size,1)))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","\n","model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters = 128,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters = 256,kernel_size = (3,3),padding = 'same',activation = 'relu'))\n","model.add(MaxPool2D(pool_size = 2,strides = 2))\n","model.add(BatchNormalization())\n","\n","model.add(Flatten())\n","model.add(Dense(units = 128,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(Dropout(0.25))\n","model.add(Dense(units = 64,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","model.add(Dense(units = 32,activation = 'relu',kernel_initializer='he_normal'))\n","model.add(Dense(7,activation = 'softmax'))\n","\n","\"\"\""]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\davia\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  warnings.warn(\n"]}],"source":["model = tf.keras.models.Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n","model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","    \n","model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten()) \n","model.add(Dense(256,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","    \n","model.add(Dense(512,activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Dense(7, activation='softmax'))\n","\n","model.compile(\n","    optimizer = Adam(lr=0.0001), \n","    loss='categorical_crossentropy', \n","    metrics=['accuracy']\n","  )"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["epochs = 60\n","batch_size = 64"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 48, 48, 32)        320       \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 48, 48, 64)        18496     \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 48, 48, 64)        256       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 24, 24, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 24, 24, 128)       204928    \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 12, 12, 512)       590336    \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 6, 6, 512)         0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 6, 6, 512)         2359808   \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 4608)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               1179904   \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 7)                 3591      \n","=================================================================\n","Total params: 4,496,903\n","Trainable params: 4,492,935\n","Non-trainable params: 3,968\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","359/359 [==============================] - 471s 1s/step - loss: 9.2847 - accuracy: 0.1954 - val_loss: 9.3256 - val_accuracy: 0.1718\n","Epoch 2/60\n","254/359 [====================>.........] - ETA: 1:48 - loss: 8.3808 - accuracy: 0.2243"]}],"source":["history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig , ax = plt.subplots(1,2)\n","train_acc = history.history['accuracy']\n","train_loss = history.history['loss']\n","fig.set_size_inches(12,4)\n","\n","ax[0].plot(history.history['accuracy'])\n","ax[0].plot(history.history['val_accuracy'])\n","ax[0].set_title('Training Accuracy vs Validation Accuracy')\n","ax[0].set_ylabel('Accuracy')\n","ax[0].set_xlabel('Epoch')\n","ax[0].legend(['Train', 'Validation'], loc='upper left')\n","\n","ax[1].plot(history.history['loss'])\n","ax[1].plot(history.history['val_loss'])\n","ax[1].set_title('Training Loss vs Validation Loss')\n","ax[1].set_ylabel('Loss')\n","ax[1].set_xlabel('Epoch')\n","ax[1].legend(['Train', 'Validation'], loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save('model_optimal.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img = image.load_img(\"../input/emotion-detection-fer/test/happy/im1021.png\",target_size = (48,48),color_mode = \"grayscale\")\n","img = np.array(img)\n","plt.imshow(img)\n","print(img.shape) #prints (48,48) that is the shape of our image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_dict = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img = np.expand_dims(img,axis = 0) #makes image shape (1,48,48)\n","img = img.reshape(1,48,48,1)\n","result = model.predict(img)\n","result = list(result[0])\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img_index = result.index(max(result))\n","print(label_dict[img_index])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loss, train_acc = model.evaluate(train_generator)\n","test_loss, test_acc   = model.evaluate(validation_generator)\n","print(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_weights('model_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.5 ('cis-face-detection')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"c9ee5cdb86aeccfe3ee3f4969d9d8a8eb448208d5b9b41c26eec8bda6963fc22"}}},"nbformat":4,"nbformat_minor":4}
